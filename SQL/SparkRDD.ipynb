{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13e9359-b42e-4135-a1fb-323133387430",
   "metadata": {},
   "source": [
    "# spark 기본개념 잡기 :RDD\n",
    "+ 여러 분사논드에 걸쳐 저장하는 변경불가 데이터 집합을 의미\n",
    "+ RDD 생성은 직접 만들거나 파일을 통해 생성할 수 있음\n",
    "+ Rdd는 transformation과 action으로 구성\n",
    "    - 기존 Rdd의 데이터를 토대로 새로운 RDD를 만들어냄\n",
    "    - RDD 기반으로 무언가를 계산해서 결과를 만들어 냄\n",
    "+ RDD는 Lazy 로딩 방식을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c01183-9c78-4a66-89dc-8cca0aec2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('/usr/share/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386a7934-a00a-4c22-8768-143c447ee783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/share/spark/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c406dbd8-bf90-4ff1-986d-ae08537b5979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " '',\n",
       " '<http://spark.apache.org/>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3565885d-994a-409b-a817-44be30fc5f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75279dde-8f9a-478e-b803-bc170c747e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " '## Building Spark',\n",
       " 'Spark is built using [Apache Maven](http://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인들 중에서 Spark 라는 단어를 찾음\n",
    "\n",
    "filterLines = lines.filter(lambda x: \"Spark\" in x)\n",
    "\n",
    "filterLines.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0cfc0-4d92-4ac9-9228-c35a190ec788",
   "metadata": {},
   "source": [
    "## Lazy 로딩방식 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8224089b-45c2-4488-8b9b-ce0f1fbce211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못된 위치의 파일을 읽어들일려고 시도\n",
    "# 현재위치 : /home/hadoop\n",
    "\n",
    "lines = sc.textFile(\"usr/share/spark/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d7125-9cae-4446-b826-af96f0b0bf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 잘못된 경로로 오류발생 - 이전 라인에서 오류가 출력되어야 하는데\n",
    "# lazy loading으로 인해 action이 호출되어야 비로소 오류가 출력\n",
    "\n",
    "lines.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac9630-1a0b-46e0-a8f7-1f3a79214367",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RDD 생성\n",
    "+ 직접 생성한 데이터로 만들거나\n",
    "    - sc.parallelize(리스트)\n",
    "+ 외부 데이터로 만드는 방법 존재\n",
    "    - sc.textFile(경로/파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc9c471-18a8-41b1-a208-c5753c4970c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['Hello, World!!', 'Hello, Python!!', 'this is RDD'])\n",
    "\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ea196-ebaa-48be-bafd-a85a46d7e503",
   "metadata": {},
   "source": [
    "### 직책별 사원수를 조회하는 spark rdd 코드를 작성해보기\n",
    "+ employees.csv를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3eeb6d8-0e23-4014-850f-5dab4d36212e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID',\n",
       " '100,Steven,King,SKING,515.123.4567,2003-06-17,AD_PRES,24000,,,90',\n",
       " '101,Neena,Kochhar,NKOCHHAR,515.123.4568,2005-09-21,AD_VP,17000,,100,90',\n",
       " '102,Lex,De Haan,LDEHAAN,515.123.4569,2001-01-13,AD_VP,17000,,100,90',\n",
       " '103,Alexander,Hunold,AHUNOLD,590.423.4567,2006-01-03,IT_PROG,9000,,102,60']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = sc.textFile('employees.csv')\n",
    "\n",
    "emp.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a8ce3d-552b-4113-b751-218e95fac693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤더를 제외하고 데이터만 골라냄\n",
    "\n",
    "header = emp.first()\n",
    "\n",
    "emp = emp.filter(lambda x: header != x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d427cc4b-71fb-4d14-954b-d79613210098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100,Steven,King,SKING,515.123.4567,2003-06-17,AD_PRES,24000,,,90'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27bc153c-ac5f-4a8b-9c6e-6def35295b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 'Steven'),\n",
       " ('AD_VP', 'Neena'),\n",
       " ('AD_VP', 'Lex'),\n",
       " ('IT_PROG', 'Alexander'),\n",
       " ('IT_PROG', 'Bruce')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사원 데이터에서 , 로 각 컬럼을 분리(split)한 후\n",
    "# 이름과 직책을 추출해냄\n",
    "\n",
    "emp2 = emp.map(lambda x: (x.split(',')[6], x.split(',')[1]))\n",
    "\n",
    "emp2.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c16b452-a048-4f20-8a9a-104e23f48bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 1), ('AD_VP', 1), ('AD_VP', 1), ('IT_PROG', 1), ('IT_PROG', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추출한 데이터에서 직책을 1로 매핑함\n",
    "maps = emp2.mapValues(lambda x: 1)\n",
    "maps.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01079f42-b7f2-4f03-a8df-11d41e83c4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 1),\n",
       " ('AD_VP', 2),\n",
       " ('IT_PROG', 5),\n",
       " ('FI_MGR', 1),\n",
       " ('FI_ACCOUNT', 5),\n",
       " ('PU_MAN', 1),\n",
       " ('PU_CLERK', 5),\n",
       " ('ST_MAN', 5),\n",
       " ('ST_CLERK', 20),\n",
       " ('SA_MAN', 5),\n",
       " ('SA_REP', 30),\n",
       " ('SH_CLERK', 20),\n",
       " ('AD_ASST', 1),\n",
       " ('MK_MAN', 1),\n",
       " ('MK_REP', 1),\n",
       " ('HR_REP', 1),\n",
       " ('PR_REP', 1),\n",
       " ('AC_MGR', 1),\n",
       " ('AC_ACCOUNT', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 직책끼리 모아서 집계처리함 (reduce)\n",
    "reduces = maps.reduceByKey(lambda x, y: x + y)\n",
    "reduces.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0304fc4-3c18-47d8-afd9-3c1de0e6b3ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 타이타닉 승객의 생존자/사망자 수를 조회하는 spark rdd 코드를 작성하세요\n",
    "+ titanic.csv를 이용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e192436d-f02c-400d-a390-12c58a74d75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pclass,survived,name,sex,age,sibsp,parch,ticket,fare,embarked,life,seat,port',\n",
       " '1,1,\"Allen, Miss. Elisabeth Walton\",female,29.0,0,0,24160,211.3375,S,live,1st,cherbourg',\n",
       " '1,1,\"Allison, Master. Hudson Trevor\",male,0.9167,1,2,113781,151.55,S,live,1st,cherbourg',\n",
       " '1,0,\"Allison, Miss. Helen Loraine\",female,2.0,1,2,113781,151.55,S,dead,1st,cherbourg',\n",
       " '1,0,\"Allison, Mr. Hudson Joshua Creighton\",male,30.0,1,2,113781,151.55,S,dead,1st,cherbourg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = sc.textFile('titanic.csv')\n",
    "\n",
    "titanic.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8282bd45-5dcb-4363-a1ef-029f7dcf9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = titanic.first()\n",
    "\n",
    "titanic = titanic.filter(lambda x: header != x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874a3403-5ca8-4dc0-8091-f174d462ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,1,\"Allen, Miss. Elisabeth Walton\",female,29.0,0,0,24160,211.3375,S,live,1st,cherbourg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c5af2b-28e9-4b73-80f8-b845e45cb16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '\"Allen'),\n",
       " ('1', '\"Allison'),\n",
       " ('0', '\"Allison'),\n",
       " ('0', '\"Allison'),\n",
       " ('0', '\"Allison')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic2 = titanic.map(lambda x: (x.split(',')[1], x.split(',')[2]))\n",
    "\n",
    "titanic2.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f68edfe-3c33-437d-aa35-47576959e082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1), ('1', 1), ('0', 1), ('0', 1), ('0', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps1 = titanic2.mapValues(lambda x: 1)\n",
    "maps1.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd1b330a-0484-46b3-9bdf-0746084f46fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 498), ('0', 808)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic1 = maps1.reduceByKey(lambda x, y: x + y)\n",
    "titanic1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e3ac3-aa59-4de9-8a22-57386dee71be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c80a4a-f6be-4029-be3e-75be7ad4a3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d10e4-bad8-4ce3-b0b7-ebad3002b513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8236680-82b6-4d18-ae46-c8a7f261d73e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
